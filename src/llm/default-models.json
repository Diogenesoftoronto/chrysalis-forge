{
  "version": "1.0",
  "description": "Default model capabilities for Chrysalis Forge",
  "sources": [
    "https://models.dev/",
    "https://openrouter.ai/api/v1/models"
  ],
  "models": [
    {
      "id": "gpt-5.2",
      "aliases": ["gpt-5", "gpt-5.2-turbo"],
      "provider": "openai",
      "family": "gpt5",
      "max_context": 128000,
      "reasoning": 0.95,
      "coding": 0.95,
      "speed": 0.7,
      "cost_tier": "expensive",
      "supports_tools": true,
      "supports_vision": true,
      "best_for": ["implement", "debug", "refactor"],
      "description": "Most capable GPT model"
    },
    {
      "id": "gpt-4o",
      "aliases": ["gpt-4o-2024-11-20"],
      "provider": "openai",
      "family": "gpt4o",
      "max_context": 128000,
      "reasoning": 0.88,
      "coding": 0.88,
      "speed": 0.85,
      "cost_tier": "moderate",
      "supports_tools": true,
      "supports_vision": true,
      "best_for": ["implement", "debug", "document"],
      "description": "Fast multimodal GPT-4 variant"
    },
    {
      "id": "gpt-4o-mini",
      "aliases": ["gpt-4o-mini-2024-07-18"],
      "provider": "openai",
      "family": "gpt4o-mini",
      "max_context": 128000,
      "reasoning": 0.75,
      "coding": 0.75,
      "speed": 1.0,
      "cost_tier": "cheap",
      "supports_tools": true,
      "supports_vision": true,
      "best_for": ["decompose", "document", "research"],
      "description": "Fast, cost-effective GPT-4o variant"
    },
    {
      "id": "gpt-4-turbo",
      "aliases": ["gpt-4-turbo-2024-04-09", "gpt-4-turbo-preview"],
      "provider": "openai",
      "family": "gpt4",
      "max_context": 128000,
      "reasoning": 0.85,
      "coding": 0.85,
      "speed": 0.6,
      "cost_tier": "expensive",
      "supports_tools": true,
      "supports_vision": true,
      "best_for": ["implement", "debug"],
      "description": "GPT-4 Turbo with vision"
    },
    {
      "id": "o1-preview",
      "aliases": ["o1-preview-2024-09-12"],
      "provider": "openai",
      "family": "o1",
      "max_context": 128000,
      "reasoning": 0.98,
      "coding": 0.85,
      "speed": 0.3,
      "cost_tier": "expensive",
      "supports_tools": false,
      "supports_vision": false,
      "best_for": ["debug", "research", "refactor"],
      "description": "Reasoning-optimized model for complex problems"
    },
    {
      "id": "o1-mini",
      "aliases": ["o1-mini-2024-09-12"],
      "provider": "openai",
      "family": "o1-mini",
      "max_context": 128000,
      "reasoning": 0.90,
      "coding": 0.80,
      "speed": 0.5,
      "cost_tier": "moderate",
      "supports_tools": false,
      "supports_vision": false,
      "best_for": ["debug", "research"],
      "description": "Faster reasoning model"
    },
    {
      "id": "o3-mini",
      "aliases": ["o3-mini-2025-01-31"],
      "provider": "openai",
      "family": "o3",
      "max_context": 200000,
      "reasoning": 0.95,
      "coding": 0.92,
      "speed": 0.6,
      "cost_tier": "moderate",
      "supports_tools": true,
      "supports_vision": false,
      "best_for": ["debug", "implement", "refactor"],
      "description": "Next-gen reasoning model with tool support"
    },
    {
      "id": "claude-4-opus",
      "aliases": ["claude-4-opus-20250514", "anthropic/claude-4-opus"],
      "provider": "anthropic",
      "family": "claude-4",
      "max_context": 200000,
      "reasoning": 0.96,
      "coding": 0.94,
      "speed": 0.5,
      "cost_tier": "expensive",
      "supports_tools": true,
      "supports_vision": true,
      "best_for": ["implement", "debug", "refactor"],
      "description": "Most capable Claude model"
    },
    {
      "id": "claude-4-sonnet",
      "aliases": ["claude-4-sonnet-20250514", "anthropic/claude-4-sonnet"],
      "provider": "anthropic",
      "family": "claude-4",
      "max_context": 200000,
      "reasoning": 0.93,
      "coding": 0.92,
      "speed": 0.75,
      "cost_tier": "moderate",
      "supports_tools": true,
      "supports_vision": true,
      "best_for": ["implement", "debug", "document"],
      "description": "Balanced Claude 4 variant"
    },
    {
      "id": "claude-3.5-sonnet",
      "aliases": ["claude-3-5-sonnet-20241022", "anthropic/claude-3.5-sonnet"],
      "provider": "anthropic",
      "family": "claude-3.5",
      "max_context": 200000,
      "reasoning": 0.90,
      "coding": 0.90,
      "speed": 0.8,
      "cost_tier": "moderate",
      "supports_tools": true,
      "supports_vision": true,
      "best_for": ["implement", "debug", "refactor"],
      "description": "High-performance Claude 3.5"
    },
    {
      "id": "claude-3.5-haiku",
      "aliases": ["claude-3-5-haiku-20241022", "anthropic/claude-3.5-haiku"],
      "provider": "anthropic",
      "family": "claude-3.5-haiku",
      "max_context": 200000,
      "reasoning": 0.78,
      "coding": 0.78,
      "speed": 1.0,
      "cost_tier": "cheap",
      "supports_tools": true,
      "supports_vision": true,
      "best_for": ["decompose", "document", "research"],
      "description": "Fast, efficient Claude variant"
    },
    {
      "id": "llama-3.3-70b",
      "aliases": ["meta-llama/llama-3.3-70b-instruct", "llama-3.3-70b-instruct"],
      "provider": "openrouter",
      "family": "llama-3",
      "max_context": 131072,
      "reasoning": 0.82,
      "coding": 0.80,
      "speed": 0.85,
      "cost_tier": "cheap",
      "supports_tools": true,
      "supports_vision": false,
      "best_for": ["implement", "document", "decompose"],
      "description": "Open-weight Llama 3.3 70B"
    },
    {
      "id": "deepseek-v3",
      "aliases": ["deepseek/deepseek-chat", "deepseek-chat"],
      "provider": "openrouter",
      "family": "deepseek",
      "max_context": 65536,
      "reasoning": 0.88,
      "coding": 0.92,
      "speed": 0.8,
      "cost_tier": "cheap",
      "supports_tools": true,
      "supports_vision": false,
      "best_for": ["implement", "refactor", "debug"],
      "description": "DeepSeek V3 - strong coding model"
    },
    {
      "id": "deepseek-r1",
      "aliases": ["deepseek/deepseek-reasoner", "deepseek-reasoner"],
      "provider": "openrouter",
      "family": "deepseek-r1",
      "max_context": 65536,
      "reasoning": 0.94,
      "coding": 0.90,
      "speed": 0.4,
      "cost_tier": "cheap",
      "supports_tools": false,
      "supports_vision": false,
      "best_for": ["debug", "research", "refactor"],
      "description": "DeepSeek R1 reasoning model"
    },
    {
      "id": "qwen-2.5-coder-32b",
      "aliases": ["qwen/qwen-2.5-coder-32b-instruct", "qwen-2.5-coder"],
      "provider": "openrouter",
      "family": "qwen-coder",
      "max_context": 131072,
      "reasoning": 0.80,
      "coding": 0.88,
      "speed": 0.9,
      "cost_tier": "cheap",
      "supports_tools": true,
      "supports_vision": false,
      "best_for": ["implement", "refactor", "debug"],
      "description": "Qwen 2.5 Coder - specialized for code"
    },
    {
      "id": "qwen-2.5-72b",
      "aliases": ["qwen/qwen-2.5-72b-instruct"],
      "provider": "openrouter",
      "family": "qwen",
      "max_context": 131072,
      "reasoning": 0.85,
      "coding": 0.82,
      "speed": 0.8,
      "cost_tier": "cheap",
      "supports_tools": true,
      "supports_vision": false,
      "best_for": ["implement", "document", "research"],
      "description": "Qwen 2.5 72B general purpose"
    },
    {
      "id": "gemini-2.0-flash",
      "aliases": ["google/gemini-2.0-flash-001"],
      "provider": "google",
      "family": "gemini-2",
      "max_context": 1048576,
      "reasoning": 0.85,
      "coding": 0.85,
      "speed": 0.95,
      "cost_tier": "cheap",
      "supports_tools": true,
      "supports_vision": true,
      "best_for": ["decompose", "document", "research"],
      "description": "Fast Gemini 2.0 with huge context"
    },
    {
      "id": "gemini-2.5-pro",
      "aliases": ["google/gemini-2.5-pro-preview"],
      "provider": "google",
      "family": "gemini-2.5",
      "max_context": 1048576,
      "reasoning": 0.92,
      "coding": 0.90,
      "speed": 0.7,
      "cost_tier": "moderate",
      "supports_tools": true,
      "supports_vision": true,
      "best_for": ["implement", "debug", "research"],
      "description": "Gemini 2.5 Pro with extended thinking"
    }
  ],
  "families": {
    "gpt5": {"reasoning": 0.95, "coding": 0.95},
    "gpt4o": {"reasoning": 0.88, "coding": 0.88},
    "gpt4o-mini": {"reasoning": 0.75, "coding": 0.75},
    "gpt4": {"reasoning": 0.85, "coding": 0.85},
    "o1": {"reasoning": 0.98, "coding": 0.85},
    "o1-mini": {"reasoning": 0.90, "coding": 0.80},
    "o3": {"reasoning": 0.95, "coding": 0.92},
    "claude-4": {"reasoning": 0.95, "coding": 0.93},
    "claude-3.5": {"reasoning": 0.90, "coding": 0.90},
    "claude-3.5-haiku": {"reasoning": 0.78, "coding": 0.78},
    "llama-3": {"reasoning": 0.82, "coding": 0.80},
    "deepseek": {"reasoning": 0.88, "coding": 0.92},
    "deepseek-r1": {"reasoning": 0.94, "coding": 0.90},
    "qwen": {"reasoning": 0.85, "coding": 0.82},
    "qwen-coder": {"reasoning": 0.80, "coding": 0.88},
    "gemini-2": {"reasoning": 0.85, "coding": 0.85},
    "gemini-2.5": {"reasoning": 0.92, "coding": 0.90}
  },
  "inference_rules": [
    {"pattern": "*-mini", "speed": 1.0, "cost_tier": "cheap"},
    {"pattern": "*-micro", "speed": 1.0, "cost_tier": "cheap", "reasoning": 0.6, "coding": 0.6},
    {"pattern": "*-nano", "speed": 1.0, "cost_tier": "cheap", "reasoning": 0.5, "coding": 0.5},
    {"pattern": "o1-*", "supports_tools": false, "supports_vision": false, "reasoning": 0.95},
    {"pattern": "o3-*", "supports_tools": true, "reasoning": 0.95},
    {"pattern": "*-vision*", "supports_vision": true},
    {"pattern": "gpt-4o*", "supports_tools": true, "supports_vision": true},
    {"pattern": "gpt-5*", "supports_tools": true, "supports_vision": true, "reasoning": 0.95, "coding": 0.95},
    {"pattern": "claude-4*", "supports_tools": true, "supports_vision": true, "reasoning": 0.93},
    {"pattern": "claude-3*", "supports_tools": true, "supports_vision": true},
    {"pattern": "*-haiku*", "speed": 1.0, "cost_tier": "cheap"},
    {"pattern": "*-opus*", "speed": 0.5, "cost_tier": "expensive", "reasoning": 0.95},
    {"pattern": "*-sonnet*", "speed": 0.75, "cost_tier": "moderate"},
    {"pattern": "llama-*-70b*", "reasoning": 0.82, "coding": 0.80, "cost_tier": "cheap"},
    {"pattern": "llama-*-405b*", "reasoning": 0.88, "coding": 0.85, "cost_tier": "moderate"},
    {"pattern": "llama-*-8b*", "reasoning": 0.65, "coding": 0.65, "speed": 1.0, "cost_tier": "cheap"},
    {"pattern": "deepseek-r1*", "supports_tools": false, "reasoning": 0.94},
    {"pattern": "deepseek*", "coding": 0.90, "cost_tier": "cheap"},
    {"pattern": "qwen*coder*", "coding": 0.88, "cost_tier": "cheap"},
    {"pattern": "qwen*", "cost_tier": "cheap"},
    {"pattern": "gemini*flash*", "speed": 0.95, "cost_tier": "cheap"},
    {"pattern": "gemini*pro*", "speed": 0.7, "cost_tier": "moderate"},
    {"pattern": "gemini*ultra*", "speed": 0.5, "cost_tier": "expensive", "reasoning": 0.95},
    {"pattern": "mistral*large*", "reasoning": 0.85, "coding": 0.82, "cost_tier": "moderate"},
    {"pattern": "mistral*small*", "speed": 0.95, "cost_tier": "cheap"},
    {"pattern": "codestral*", "coding": 0.88, "cost_tier": "cheap"},
    {"pattern": "*-instruct", "supports_tools": true},
    {"pattern": "*-chat", "supports_tools": true},
    {"pattern": "*-turbo", "speed": 0.85},
    {"pattern": "*-preview", "cost_tier": "moderate"},
    {"pattern": "*-experimental", "cost_tier": "cheap"}
  ],
  "task_type_weights": {
    "implement": {"coding": 1.0, "reasoning": 0.7, "speed": 0.3},
    "debug": {"reasoning": 1.0, "coding": 0.8, "speed": 0.2},
    "research": {"reasoning": 0.9, "coding": 0.3, "speed": 0.5},
    "refactor": {"coding": 1.0, "reasoning": 0.6, "speed": 0.4},
    "decompose": {"speed": 1.0, "reasoning": 0.5, "coding": 0.2},
    "document": {"reasoning": 0.6, "coding": 0.4, "speed": 0.7},
    "review": {"reasoning": 0.9, "coding": 0.7, "speed": 0.3},
    "test": {"coding": 0.9, "reasoning": 0.6, "speed": 0.4}
  },
  "cost_tiers": {
    "cheap": {"max_input_cost_per_mtok": 0.5, "max_output_cost_per_mtok": 2.0},
    "moderate": {"max_input_cost_per_mtok": 5.0, "max_output_cost_per_mtok": 15.0},
    "expensive": {"max_input_cost_per_mtok": 30.0, "max_output_cost_per_mtok": 120.0}
  }
}
